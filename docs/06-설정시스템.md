# 6. 설정 시스템 (Configuration System)

## 목차
1. [개요](#개요)
2. [ConfigLoader - 설정 로더](#configloader---설정-로더)
3. [3-Tier 설정 구조](#3-tier-설정-구조)
4. [LLM 설정 방식](#llm-설정-방식)
5. [환경 변수](#환경-변수)
6. [설정 검증](#설정-검증)
7. [실전 예시](#실전-예시)

---

## 개요

GAI의 설정 시스템은 **3-tier 구조**로 되어 있어 유연한 설정 관리가 가능합니다.

**3-Tier 구조**:
1. **Default Config** (최하위) - 코드에 하드코딩된 기본값
2. **User Config File** (중간) - `config/config.json` 또는 `config.json`
3. **Environment Variables** (최우선) - `.env` 파일 또는 시스템 환경 변수

**우선순위**: `Environment Variables > User Config File > Default Config`

**핵심 파일**:
- `src/config/ConfigLoader.ts` (300 lines) - 설정 로딩 및 병합 로직

---

## ConfigLoader - 설정 로더

### 파일 위치
- `src/config/ConfigLoader.ts` (300 lines)

### 목적
**3-tier 설정을 로딩하고 병합**하여 최종 Config 객체를 생성합니다. API Key 주입, 검증 기능도 제공합니다.

### 생성자

```typescript
export class ConfigLoader {
  private config: Config;

  constructor(configPath?: string) {
    // 1. .env 파일 로드
    dotenvConfig();

    // 2. 설정 로드 (3-tier 병합)
    this.config = this.loadConfig(configPath);
  }
}
```

**생성자 동작**:
1. `.env` 파일 로드 (dotenv 사용)
2. `loadConfig()` 호출 → 3-tier 병합

### Public API

#### 1) `get(): Config`

**목적**: 로드된 설정 객체 반환

```typescript
get(): Config {
  return this.config;
}
```

**사용 예시**:
```typescript
const configLoader = new ConfigLoader();
const config = configLoader.get();
console.log(config.llm.provider); // 'anthropic'
```

#### 2) `validate(): { valid: boolean; errors: string[] }`

**목적**: 설정 검증 (필수 필드, API key 등)

```typescript
validate(): { valid: boolean; errors: string[] } {
  const errors: string[] = [];

  // LLM config 검증
  if (this.isModeConfig(this.config.llm)) {
    // Mode config: default, fast, vision 각각 검증
    const modeConfig = this.config.llm as LLMModeConfig;
    errors.push(...this.validateLLMConfig(modeConfig.default, 'default'));
    errors.push(...this.validateLLMConfig(modeConfig.fast, 'fast'));
    errors.push(...this.validateLLMConfig(modeConfig.vision, 'vision'));
  } else {
    // Single config 검증
    errors.push(...this.validateLLMConfig(this.config.llm as LLMConfig));
  }

  // Telegram config 검증
  if (this.config.telegram.enabled && !this.config.telegram.botToken) {
    errors.push('Telegram bot token required when Telegram is enabled');
  }

  return {
    valid: errors.length === 0,
    errors,
  };
}
```

**검증 항목**:
- LLM provider 필수
- API key 필수 (ollama 제외)
- model 필수
- Telegram botToken (Telegram 활성화 시)

**반환 예시**:
```typescript
// 성공
{ valid: true, errors: [] }

// 실패
{
  valid: false,
  errors: [
    'LLM config (default): API key required for anthropic',
    'LLM config (vision): model not specified'
  ]
}
```

---

## 3-Tier 설정 구조

### 설정 로딩 흐름

```typescript
private loadConfig(configPath?: string): Config {
  // 1. Default config 로드
  const defaultConfig = this.getDefaultConfig();

  // 2. User config file 로드
  const userConfig = this.loadConfigFile(configPath);

  // 3. Environment variables 로드
  const envConfig = this.loadEnvConfig();

  // 4. 3개 config 병합 (env가 최우선)
  const merged = this.mergeConfigs(defaultConfig, userConfig, envConfig);

  // 5. API keys 주입 (환경 변수에서)
  this.injectApiKeys(merged);

  return merged;
}
```

**단계별 설명**:
1. **Default Config**: `getDefaultConfig()` - 하드코딩된 기본값
2. **User Config**: `loadConfigFile()` - JSON 파일 읽기
3. **Env Config**: `loadEnvConfig()` - 환경 변수 파싱
4. **병합**: `mergeConfigs()` - Deep merge (env > user > default)
5. **API Key 주입**: `injectApiKeys()` - ANTHROPIC_API_KEY 등 주입

### Tier 1: Default Config

```typescript
private getDefaultConfig(): Config {
  return {
    llm: {
      provider: 'ollama',
      model: '',
      maxTokens: 8096,
      temperature: 0.7,
    },
    telegram: {
      enabled: false,
    },
    tools: {
      gui: {
        enabled: true,
        screenCaptureInterval: 1000,
      },
      builtin: {
        filesystem: true,
        websearch: false,
        codeExecutor: false,
      },
    },
    agent: {
      maxIterations: 10,
    },
  };
}
```

**기본값 특징**:
- **LLM**: Ollama (API key 불필요)
- **Telegram**: 비활성화
- **Tools**: GUI 도구만 활성화
- **Agent**: 최대 10회 반복

### Tier 2: User Config File

```typescript
private loadConfigFile(configPath?: string): Partial<Config> {
  // 우선순위 순서로 경로 탐색
  const paths = [
    configPath,                                    // 1. 명시적 경로
    path.join(process.cwd(), 'config', 'config.json'), // 2. config/config.json
    path.join(process.cwd(), 'config.json'),           // 3. config.json
  ].filter((p): p is string => p !== undefined);

  // 첫 번째로 존재하는 파일 로드
  for (const p of paths) {
    if (fs.existsSync(p)) {
      try {
        const content = fs.readFileSync(p, 'utf-8');
        return JSON.parse(content);
      } catch (error) {
        console.warn(`Failed to load config from ${p}:`, error);
      }
    }
  }

  // 파일이 없으면 빈 객체 반환
  return {};
}
```

**파일 탐색 순서**:
1. 생성자로 전달된 `configPath`
2. `{project}/config/config.json`
3. `{project}/config.json`

**JSON 파일 예시** (`config/config.json`):
```json
{
  "llm": {
    "default": {
      "provider": "anthropic",
      "model": "claude-sonnet-4-5-20250929",
      "maxTokens": 8096,
      "temperature": 0.7
    },
    "fast": {
      "provider": "anthropic",
      "model": "claude-haiku-4-5-20251001",
      "maxTokens": 4096,
      "temperature": 0.5
    },
    "vision": {
      "provider": "anthropic",
      "model": "claude-opus-4-6",
      "maxTokens": 8096,
      "temperature": 0.7
    }
  },
  "agent": {
    "maxIterations": 20
  }
}
```

**특징**:
- JSON 형식
- Partial<Config> - 일부 필드만 오버라이드 가능
- 파싱 실패 시 경고 출력 후 다음 경로 시도

### Tier 3: Environment Variables

```typescript
private loadEnvConfig(): Partial<Config> {
  const envConfig: Partial<Config> = {};

  // ===== LLM Config =====
  // DEFAULT_PROVIDER가 있으면 모드 설정으로 처리
  if (process.env.DEFAULT_PROVIDER) {
    const defaultConfig = this.loadLLMConfigFromEnv('DEFAULT');
    const fastConfig = this.loadLLMConfigFromEnv('FAST');
    const visionConfig = this.loadLLMConfigFromEnv('VISION');

    envConfig.llm = {
      default: defaultConfig,
      // FAST/VISION 설정 없으면 DEFAULT 재사용
      fast: fastConfig.provider ? fastConfig : defaultConfig,
      vision: visionConfig.provider ? visionConfig : defaultConfig,
    } as LLMModeConfig;
  }

  // ===== Agent Config =====
  if (process.env.AGENT_MAX_ITERATIONS) {
    envConfig.agent = {
      maxIterations: parseInt(process.env.AGENT_MAX_ITERATIONS, 10),
    };
  }

  // ===== Telegram Config =====
  if (process.env.TELEGRAM_ENABLED || process.env.TELEGRAM_BOT_TOKEN) {
    envConfig.telegram = {
      enabled: process.env.TELEGRAM_ENABLED === 'true',
      botToken: process.env.TELEGRAM_BOT_TOKEN,
    };
  }

  return envConfig;
}
```

**주요 로직**:
1. **LLM Multi-Mode**: `DEFAULT_PROVIDER` 존재 시 → default, fast, vision 각각 로드
2. **Agent**: `AGENT_MAX_ITERATIONS` → 정수 파싱
3. **Telegram**: `TELEGRAM_ENABLED`, `TELEGRAM_BOT_TOKEN`

**LLM Config 로딩** (Prefix별):
```typescript
private loadLLMConfigFromEnv(prefix: string): LLMConfig {
  const config: Partial<LLMConfig> = {};

  // {PREFIX}_PROVIDER
  const provider = process.env[`${prefix}_PROVIDER`];
  if (provider) {
    config.provider = provider as LLMProvider;
  }

  // {PREFIX}_MODEL
  const model = process.env[`${prefix}_MODEL`];
  if (model) {
    config.model = model;
  }

  // {PREFIX}_MAX_TOKENS
  const maxTokens = process.env[`${prefix}_MAX_TOKENS`];
  if (maxTokens) {
    config.maxTokens = parseInt(maxTokens, 10);
  }

  // {PREFIX}_TEMPERATURE
  const temperature = process.env[`${prefix}_TEMPERATURE`];
  if (temperature) {
    config.temperature = parseFloat(temperature);
  }

  return config as LLMConfig;
}
```

**Prefix 종류**:
- `DEFAULT_*`: default mode
- `FAST_*`: fast mode
- `VISION_*`: vision mode

**Fallback 로직**:
```typescript
fast: fastConfig.provider ? fastConfig : defaultConfig,
vision: visionConfig.provider ? visionConfig : defaultConfig,
```
- FAST/VISION 설정이 없으면 DEFAULT 설정 재사용

### 설정 병합 (Deep Merge)

```typescript
private mergeConfigs(...configs: Array<Partial<Config>>): Config {
  let result: Partial<Config> = {};

  for (const config of configs) {
    result = this.deepMerge(result, config);
  }

  return result as Config;
}
```

**호출 예시**:
```typescript
mergeConfigs(defaultConfig, userConfig, envConfig)
```

**Deep Merge 로직**:
```typescript
private deepMerge<T extends Record<string, unknown>>(target: T, source: T): T {
  const output = { ...target };

  for (const key in source) {
    const sourceValue = source[key];
    const targetValue = target[key];

    // 둘 다 객체면 재귀적으로 병합
    if (this.isObject(sourceValue) && this.isObject(targetValue)) {
      output[key] = this.deepMerge(
        targetValue as Record<string, unknown>,
        sourceValue as Record<string, unknown>
      ) as T[Extract<keyof T, string>];
    } else {
      // 아니면 source 값으로 덮어쓰기
      output[key] = sourceValue;
    }
  }

  return output;
}
```

**예시**:
```typescript
// target
{
  llm: { provider: 'ollama', model: '', maxTokens: 8096 },
  agent: { maxIterations: 10 }
}

// source
{
  llm: { provider: 'anthropic', model: 'claude-sonnet-4-5' },
  agent: { maxIterations: 20 }
}

// result (deep merge)
{
  llm: { provider: 'anthropic', model: 'claude-sonnet-4-5', maxTokens: 8096 },
  agent: { maxIterations: 20 }
}
```

**특징**:
- **중첩 객체 병합**: llm 내부의 provider만 덮어쓰고 maxTokens는 유지
- **배열은 덮어쓰기**: Deep merge 하지 않고 source로 교체

---

## LLM 설정 방식

GAI는 **두 가지 LLM 설정 방식**을 지원합니다.

### Single Mode (단일 LLM)

**특징**: 모든 요청에 동일한 LLM 사용

**Config 타입**:
```typescript
interface LLMConfig {
  provider: LLMProvider; // 'anthropic' | 'openai' | 'google' | 'ollama'
  model: string;
  apiKey?: string;
  baseUrl?: string; // Ollama용
  maxTokens?: number;
  temperature?: number;
}
```

**JSON 예시**:
```json
{
  "llm": {
    "provider": "anthropic",
    "model": "claude-sonnet-4-5-20250929",
    "maxTokens": 8096,
    "temperature": 0.7
  }
}
```

**환경 변수 예시**:
```bash
LLM_PROVIDER=anthropic
LLM_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_API_KEY=sk-ant-...
```

### Multi-Mode (모드별 LLM)

**특징**: default, fast, vision 각각 다른 LLM 사용

**Config 타입**:
```typescript
interface LLMModeConfig {
  default: LLMConfig; // 기본 모드 (Agent loop)
  fast: LLMConfig;    // 빠른 모드 (단순 작업)
  vision: LLMConfig;  // Vision 모드 (이미지 분석)
}
```

**JSON 예시**:
```json
{
  "llm": {
    "default": {
      "provider": "anthropic",
      "model": "claude-sonnet-4-5-20250929"
    },
    "fast": {
      "provider": "anthropic",
      "model": "claude-haiku-4-5-20251001"
    },
    "vision": {
      "provider": "anthropic",
      "model": "claude-opus-4-6"
    }
  }
}
```

**환경 변수 예시**:
```bash
# Default mode
DEFAULT_PROVIDER=anthropic
DEFAULT_MODEL=claude-sonnet-4-5-20250929

# Fast mode
FAST_PROVIDER=anthropic
FAST_MODEL=claude-haiku-4-5-20251001

# Vision mode
VISION_PROVIDER=anthropic
VISION_MODEL=claude-opus-4-6

# API Key (모든 모드 공유)
ANTHROPIC_API_KEY=sk-ant-...
```

### Mode 구분 로직

```typescript
private isModeConfig(llm: LLMConfig | LLMModeConfig): llm is LLMModeConfig {
  return 'default' in llm && 'fast' in llm && 'vision' in llm;
}
```

**검사 방법**: `default`, `fast`, `vision` 키가 모두 존재하면 ModeConfig

---

## API Key 주입

환경 변수에서 API Key를 읽어 Config에 주입합니다.

### 주입 흐름

```typescript
private injectApiKeys(config: Config): void {
  // Mode config인지 확인
  if (this.isModeConfig(config.llm)) {
    const modeConfig = config.llm as LLMModeConfig;
    this.injectApiKeyToLLMConfig(modeConfig.default);
    this.injectApiKeyToLLMConfig(modeConfig.fast);
    this.injectApiKeyToLLMConfig(modeConfig.vision);
  } else {
    this.injectApiKeyToLLMConfig(config.llm as LLMConfig);
  }
}
```

**로직**:
- Mode config면 default, fast, vision 각각 주입
- Single config면 한 번만 주입

### API Key 주입 (단일 LLM)

```typescript
private injectApiKeyToLLMConfig(llmConfig: LLMConfig): void {
  if (!llmConfig.provider) return;

  // 1. Provider에 맞는 API Key 조회
  const apiKey = this.getApiKeyForProvider(llmConfig.provider);
  if (apiKey) {
    llmConfig.apiKey = apiKey;
  }

  // 2. Ollama baseUrl 주입
  if (llmConfig.provider === 'ollama' && process.env.OLLAMA_BASE_URL) {
    llmConfig.baseUrl = process.env.OLLAMA_BASE_URL;
  }
}
```

**단계**:
1. **API Key**: provider에 따라 적절한 환경 변수 조회
2. **Ollama baseUrl**: `OLLAMA_BASE_URL` 주입

### Provider별 API Key 매핑

```typescript
private getApiKeyForProvider(provider: LLMProvider): string | undefined {
  switch (provider) {
    case 'anthropic':
      return process.env.ANTHROPIC_API_KEY;
    case 'openai':
      return process.env.OPENAI_API_KEY;
    case 'google':
      return process.env.GOOGLE_API_KEY;
    case 'ollama':
      return undefined; // Ollama는 API key 불필요
    default:
      return undefined;
  }
}
```

**매핑 테이블**:

| Provider | 환경 변수 | 필수 여부 |
|----------|-----------|-----------|
| `anthropic` | `ANTHROPIC_API_KEY` | ✅ 필수 |
| `openai` | `OPENAI_API_KEY` | ✅ 필수 |
| `google` | `GOOGLE_API_KEY` | ✅ 필수 |
| `ollama` | (없음) | ❌ 불필요 |

**Ollama 특별 처리**:
- API key 불필요
- `OLLAMA_BASE_URL` 환경 변수 사용 (기본: `http://localhost:11434`)

---

## 환경 변수

### LLM 설정 (Single Mode)

```bash
# Provider 및 모델
LLM_PROVIDER=anthropic
LLM_MODEL=claude-sonnet-4-5-20250929

# API Key
ANTHROPIC_API_KEY=sk-ant-api03-...

# 옵션
LLM_MAX_TOKENS=8096
LLM_TEMPERATURE=0.7
```

### LLM 설정 (Multi-Mode)

```bash
# Default mode
DEFAULT_PROVIDER=anthropic
DEFAULT_MODEL=claude-sonnet-4-5-20250929

# Fast mode (선택)
FAST_PROVIDER=anthropic
FAST_MODEL=claude-haiku-4-5-20251001

# Vision mode (선택)
VISION_PROVIDER=anthropic
VISION_MODEL=claude-opus-4-6

# API Key (공통)
ANTHROPIC_API_KEY=sk-ant-api03-...
```

**주의사항**:
- `DEFAULT_PROVIDER`가 있으면 자동으로 Multi-Mode로 인식
- `FAST_*`, `VISION_*` 생략 시 `DEFAULT_*` 설정 재사용

### Ollama 설정

```bash
# Ollama (API key 불필요)
DEFAULT_PROVIDER=ollama
DEFAULT_MODEL=llama2
OLLAMA_BASE_URL=http://localhost:11434

# Fast mode도 Ollama 사용
FAST_PROVIDER=ollama
FAST_MODEL=llama2:7b
```

### Agent 설정

```bash
# 최대 반복 횟수
AGENT_MAX_ITERATIONS=20
```

### Telegram 설정

```bash
# Telegram Bot
TELEGRAM_ENABLED=true
TELEGRAM_BOT_TOKEN=123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
```

### 전체 예시 (.env)

```bash
# LLM Multi-Mode 설정
DEFAULT_PROVIDER=anthropic
DEFAULT_MODEL=claude-sonnet-4-5-20250929
DEFAULT_MAX_TOKENS=8096
DEFAULT_TEMPERATURE=0.7

FAST_PROVIDER=anthropic
FAST_MODEL=claude-haiku-4-5-20251001
FAST_MAX_TOKENS=4096
FAST_TEMPERATURE=0.5

VISION_PROVIDER=anthropic
VISION_MODEL=claude-opus-4-6
VISION_MAX_TOKENS=8096
VISION_TEMPERATURE=0.7

# API Keys
ANTHROPIC_API_KEY=sk-ant-api03-...

# Agent
AGENT_MAX_ITERATIONS=20

# Telegram (선택)
TELEGRAM_ENABLED=false
```

---

## 설정 검증

### validate() 메서드

```typescript
validate(): { valid: boolean; errors: string[] } {
  const errors: string[] = [];

  // LLM config 검증
  if (this.isModeConfig(this.config.llm)) {
    const modeConfig = this.config.llm as LLMModeConfig;
    errors.push(...this.validateLLMConfig(modeConfig.default, 'default'));
    errors.push(...this.validateLLMConfig(modeConfig.fast, 'fast'));
    errors.push(...this.validateLLMConfig(modeConfig.vision, 'vision'));
  } else {
    errors.push(...this.validateLLMConfig(this.config.llm as LLMConfig));
  }

  // Telegram config 검증
  if (this.config.telegram.enabled && !this.config.telegram.botToken) {
    errors.push('Telegram bot token required when Telegram is enabled');
  }

  return {
    valid: errors.length === 0,
    errors,
  };
}
```

### LLM Config 검증

```typescript
private validateLLMConfig(llmConfig: LLMConfig, mode?: string): string[] {
  const errors: string[] = [];
  const label = mode ? `LLM config (${mode})` : 'LLM config';

  // 1. Provider 필수
  if (!llmConfig.provider) {
    errors.push(`${label}: provider not specified`);
    return errors;
  }

  // 2. API Key 필수 (Ollama 제외)
  if (llmConfig.provider !== 'ollama' && !llmConfig.apiKey) {
    errors.push(`${label}: API key required for ${llmConfig.provider}`);
  }

  // 3. Ollama baseUrl 필수
  if (llmConfig.provider === 'ollama' && !llmConfig.baseUrl) {
    errors.push(`${label}: baseUrl required for Ollama`);
  }

  // 4. Model 필수
  if (!llmConfig.model) {
    errors.push(`${label}: model not specified`);
  }

  return errors;
}
```

**검증 항목**:
1. **provider 필수**: 'anthropic', 'openai', 'google', 'ollama' 중 하나
2. **API key 필수**: Ollama 제외한 모든 provider
3. **Ollama baseUrl**: Ollama 사용 시 필수
4. **model 필수**: 모델명 지정 필요

### 검증 실행 (index.ts)

```typescript
const configLoader = new ConfigLoader();
const config = configLoader.get();

// 검증
const validation = configLoader.validate();
if (!validation.valid) {
  console.error('Configuration validation failed:');
  validation.errors.forEach((error) => console.error(`  - ${error}`));
  process.exit(1);
}

console.log('✓ Configuration validated successfully');
```

**에러 예시**:
```
Configuration validation failed:
  - LLM config (default): API key required for anthropic
  - LLM config (vision): model not specified
  - Telegram bot token required when Telegram is enabled
```

---

## 실전 예시

### 예시 1: Anthropic Single Mode

**config/config.json**:
```json
{
  "llm": {
    "provider": "anthropic",
    "model": "claude-sonnet-4-5-20250929",
    "maxTokens": 8096,
    "temperature": 0.7
  }
}
```

**.env**:
```bash
ANTHROPIC_API_KEY=sk-ant-api03-...
```

**최종 Config**:
```typescript
{
  llm: {
    provider: 'anthropic',
    model: 'claude-sonnet-4-5-20250929',
    apiKey: 'sk-ant-api03-...',  // 환경 변수에서 주입
    maxTokens: 8096,
    temperature: 0.7
  },
  telegram: { enabled: false },
  tools: { ... },
  agent: { maxIterations: 10 }
}
```

### 예시 2: Multi-Mode (환경 변수만)

**.env**:
```bash
DEFAULT_PROVIDER=anthropic
DEFAULT_MODEL=claude-sonnet-4-5-20250929

FAST_PROVIDER=anthropic
FAST_MODEL=claude-haiku-4-5-20251001

VISION_PROVIDER=anthropic
VISION_MODEL=claude-opus-4-6

ANTHROPIC_API_KEY=sk-ant-api03-...
AGENT_MAX_ITERATIONS=20
```

**최종 Config**:
```typescript
{
  llm: {
    default: {
      provider: 'anthropic',
      model: 'claude-sonnet-4-5-20250929',
      apiKey: 'sk-ant-api03-...',
      maxTokens: 8096,
      temperature: 0.7
    },
    fast: {
      provider: 'anthropic',
      model: 'claude-haiku-4-5-20251001',
      apiKey: 'sk-ant-api03-...',
      maxTokens: 8096,  // default에서 상속
      temperature: 0.7
    },
    vision: {
      provider: 'anthropic',
      model: 'claude-opus-4-6',
      apiKey: 'sk-ant-api03-...',
      maxTokens: 8096,
      temperature: 0.7
    }
  },
  agent: { maxIterations: 20 },
  telegram: { enabled: false },
  tools: { ... }
}
```

### 예시 3: Ollama 로컬 사용

**.env**:
```bash
DEFAULT_PROVIDER=ollama
DEFAULT_MODEL=llama2
OLLAMA_BASE_URL=http://localhost:11434
```

**최종 Config**:
```typescript
{
  llm: {
    provider: 'ollama',
    model: 'llama2',
    baseUrl: 'http://localhost:11434',
    apiKey: undefined,  // Ollama는 불필요
    maxTokens: 8096,
    temperature: 0.7
  },
  // ...
}
```

### 예시 4: 3-Tier 병합

**Default Config** (코드):
```typescript
{
  llm: { provider: 'ollama', model: '', maxTokens: 8096, temperature: 0.7 },
  agent: { maxIterations: 10 }
}
```

**config/config.json**:
```json
{
  "llm": {
    "provider": "anthropic",
    "model": "claude-sonnet-4-5"
  },
  "agent": {
    "maxIterations": 15
  }
}
```

**.env**:
```bash
ANTHROPIC_API_KEY=sk-ant-...
AGENT_MAX_ITERATIONS=20
```

**최종 Config** (병합 결과):
```typescript
{
  llm: {
    provider: 'anthropic',        // config.json에서
    model: 'claude-sonnet-4-5',   // config.json에서
    apiKey: 'sk-ant-...',         // .env에서 주입
    maxTokens: 8096,              // default에서
    temperature: 0.7              // default에서
  },
  agent: {
    maxIterations: 20             // .env가 최우선
  }
}
```

**병합 우선순위**: `.env (20) > config.json (15) > default (10)`

---

## Config 타입 정의

### Config (전체 설정)

```typescript
interface Config {
  llm: LLMConfig | LLMModeConfig;
  telegram: TelegramConfig;
  tools: ToolsConfig;
  agent: AgentConfig;
}
```

### LLMConfig (단일 LLM)

```typescript
interface LLMConfig {
  provider: LLMProvider; // 'anthropic' | 'openai' | 'google' | 'ollama'
  model: string;
  apiKey?: string;
  baseUrl?: string;      // Ollama용
  maxTokens?: number;
  temperature?: number;
}
```

### LLMModeConfig (Multi-Mode)

```typescript
interface LLMModeConfig {
  default: LLMConfig;
  fast: LLMConfig;
  vision: LLMConfig;
}
```

### TelegramConfig

```typescript
interface TelegramConfig {
  enabled: boolean;
  botToken?: string;
}
```

### ToolsConfig

```typescript
interface ToolsConfig {
  gui: {
    enabled: boolean;
    screenCaptureInterval: number;
  };
  builtin: {
    filesystem: boolean;
    websearch: boolean;
    codeExecutor: boolean;
  };
}
```

### AgentConfig

```typescript
interface AgentConfig {
  maxIterations: number;
}
```

---

## 정리

### 핵심 개념

1. **3-Tier 구조**:
   - Default Config (코드) < User Config File (JSON) < Environment Variables (.env)
   - Deep merge로 병합 (중첩 객체 재귀 병합)

2. **LLM 설정 방식**:
   - **Single Mode**: 모든 요청에 동일한 LLM
   - **Multi-Mode**: default, fast, vision 각각 다른 LLM

3. **API Key 주입**:
   - 환경 변수에서 provider별로 자동 주입
   - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, `GOOGLE_API_KEY`
   - Ollama는 API key 불필요, `OLLAMA_BASE_URL` 사용

4. **검증**:
   - `validate()` 메서드로 필수 필드 확인
   - provider, model 필수
   - API key 필수 (Ollama 제외)

5. **우선순위**:
   - 환경 변수가 최우선
   - JSON 파일이 중간
   - Default config가 최하위

### 설정 파일 위치

| 우선순위 | 파일 | 설명 |
|---------|------|------|
| 1 | `.env` | 환경 변수 (최우선) |
| 2 | `config/config.json` | User config file |
| 3 | `config.json` | User config file (대체) |
| 4 | 코드 | Default config (fallback) |

### 환경 변수 요약

| 변수 | 설명 | 예시 |
|------|------|------|
| `DEFAULT_PROVIDER` | Default mode provider | `anthropic` |
| `DEFAULT_MODEL` | Default mode model | `claude-sonnet-4-5-20250929` |
| `FAST_PROVIDER` | Fast mode provider (선택) | `anthropic` |
| `FAST_MODEL` | Fast mode model (선택) | `claude-haiku-4-5-20251001` |
| `VISION_PROVIDER` | Vision mode provider (선택) | `anthropic` |
| `VISION_MODEL` | Vision mode model (선택) | `claude-opus-4-6` |
| `ANTHROPIC_API_KEY` | Anthropic API key | `sk-ant-api03-...` |
| `OPENAI_API_KEY` | OpenAI API key | `sk-...` |
| `GOOGLE_API_KEY` | Google API key | `AIza...` |
| `OLLAMA_BASE_URL` | Ollama base URL | `http://localhost:11434` |
| `AGENT_MAX_ITERATIONS` | Agent 최대 반복 | `20` |
| `TELEGRAM_ENABLED` | Telegram 활성화 | `true` |
| `TELEGRAM_BOT_TOKEN` | Telegram bot token | `123456:ABC...` |

### 파일 크기

- `ConfigLoader.ts`: 300 lines
